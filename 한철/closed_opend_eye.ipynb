{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os, glob, sys, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2586, 26, 34, 1)\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "import numpy as np\n",
    "\n",
    "## 먼저 기존의 np.load를 np_load_old에 저장해둠.\n",
    "np_load_old = np.load\n",
    "\n",
    "## 기존의 parameter을 바꿔줌\n",
    "np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k)\n",
    "\n",
    "X_train, X_test, y_train, y_test = np.load('./Pictures/imagecrowling/blink_data2.npy')\n",
    "print(X_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 26, 34, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 13, 17, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 13, 17, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 8, 32)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 6, 8, 64)          18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 3, 4, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 3, 4, 64)          36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 1, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1, 2, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 256)               33024     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 98,273\n",
      "Trainable params: 98,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "image_w = 26\n",
    "image_h = 34\n",
    "X_train = X_train.astype('float32') / 255\n",
    "X_test = X_test.astype('float32') / 255\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", input_shape=X_train.shape[1:], activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(32, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3,3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256, activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model_dir = './Pictures/imagecrowling/model'\n",
    "if not os.path.exists(model_dir):\n",
    "    os.mkdir(model_dir)\n",
    "model_path = model_dir + \"/open_close_classify2.model\"\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=model_path, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=7)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.6338 - accuracy: 0.6324\n",
      "Epoch 00001: val_loss improved from inf to 0.41598, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "WARNING:tensorflow:From C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\tracking\\tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 7s 192ms/step - loss: 0.6318 - accuracy: 0.6347 - val_loss: 0.4160 - val_accuracy: 0.8428\n",
      "Epoch 2/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.3502 - accuracy: 0.8653\n",
      "Epoch 00002: val_loss improved from 0.41598 to 0.25518, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 6s 165ms/step - loss: 0.3503 - accuracy: 0.8649 - val_loss: 0.2552 - val_accuracy: 0.8943\n",
      "Epoch 3/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.2308 - accuracy: 0.9190\n",
      "Epoch 00003: val_loss improved from 0.25518 to 0.14265, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 6s 166ms/step - loss: 0.2308 - accuracy: 0.9190 - val_loss: 0.1427 - val_accuracy: 0.9433\n",
      "Epoch 4/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.1629 - accuracy: 0.9403\n",
      "Epoch 00004: val_loss improved from 0.14265 to 0.11128, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 6s 174ms/step - loss: 0.1615 - accuracy: 0.9409 - val_loss: 0.1113 - val_accuracy: 0.9588\n",
      "Epoch 5/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.1340 - accuracy: 0.9513 ETA: 0s - loss: 0\n",
      "Epoch 00005: val_loss improved from 0.11128 to 0.08204, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 6s 165ms/step - loss: 0.1340 - accuracy: 0.9513 - val_loss: 0.0820 - val_accuracy: 0.9665\n",
      "Epoch 6/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.0996 - accuracy: 0.9674\n",
      "Epoch 00006: val_loss improved from 0.08204 to 0.06199, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 6s 166ms/step - loss: 0.0992 - accuracy: 0.9677 - val_loss: 0.0620 - val_accuracy: 0.9794\n",
      "Epoch 7/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0736 - accuracy: 0.9786\n",
      "Epoch 00007: val_loss improved from 0.06199 to 0.05363, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 6s 165ms/step - loss: 0.0736 - accuracy: 0.9786 - val_loss: 0.0536 - val_accuracy: 0.9794\n",
      "Epoch 8/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.0646 - accuracy: 0.9807\n",
      "Epoch 00008: val_loss improved from 0.05363 to 0.03233, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 6s 169ms/step - loss: 0.0659 - accuracy: 0.9804 - val_loss: 0.0323 - val_accuracy: 0.9948\n",
      "Epoch 9/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.0650 - accuracy: 0.9761\n",
      "Epoch 00009: val_loss did not improve from 0.03233\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0647 - accuracy: 0.9763 - val_loss: 0.0346 - val_accuracy: 0.9897\n",
      "Epoch 10/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0541 - accuracy: 0.9795\n",
      "Epoch 00010: val_loss did not improve from 0.03233\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0541 - accuracy: 0.9795 - val_loss: 0.0361 - val_accuracy: 0.9897\n",
      "Epoch 11/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0317 - accuracy: 0.9877\n",
      "Epoch 00011: val_loss did not improve from 0.03233\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0317 - accuracy: 0.9877 - val_loss: 0.0432 - val_accuracy: 0.9845\n",
      "Epoch 12/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0302 - accuracy: 0.9909\n",
      "Epoch 00012: val_loss did not improve from 0.03233\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0302 - accuracy: 0.9909 - val_loss: 0.0411 - val_accuracy: 0.9820\n",
      "Epoch 13/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0240 - accuracy: 0.9932\n",
      "Epoch 00013: val_loss did not improve from 0.03233\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0240 - accuracy: 0.9932 - val_loss: 0.0571 - val_accuracy: 0.9820\n",
      "Epoch 14/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0283 - accuracy: 0.9914\n",
      "Epoch 00014: val_loss improved from 0.03233 to 0.02335, saving model to ./Pictures/imagecrowling/model\\open_close_classify2.model\n",
      "INFO:tensorflow:Assets written to: ./Pictures/imagecrowling/model\\open_close_classify2.model\\assets\n",
      "35/35 [==============================] - 6s 161ms/step - loss: 0.0283 - accuracy: 0.9914 - val_loss: 0.0234 - val_accuracy: 0.9948\n",
      "Epoch 15/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.0191 - accuracy: 0.9949\n",
      "Epoch 00015: val_loss did not improve from 0.02335\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0191 - accuracy: 0.9950 - val_loss: 0.0442 - val_accuracy: 0.9820\n",
      "Epoch 16/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.0222 - accuracy: 0.9922\n",
      "Epoch 00016: val_loss did not improve from 0.02335\n",
      "35/35 [==============================] - 2s 45ms/step - loss: 0.0221 - accuracy: 0.9923 - val_loss: 0.0500 - val_accuracy: 0.9820\n",
      "Epoch 17/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0246 - accuracy: 0.9909 ETA: 1s - l\n",
      "Epoch 00017: val_loss did not improve from 0.02335\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0246 - accuracy: 0.9909 - val_loss: 0.0256 - val_accuracy: 0.9871\n",
      "Epoch 18/100\n",
      "34/35 [============================>.] - ETA: 0s - loss: 0.0243 - accuracy: 0.9931\n",
      "Epoch 00018: val_loss did not improve from 0.02335\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0242 - accuracy: 0.9932 - val_loss: 0.0309 - val_accuracy: 0.9923\n",
      "Epoch 19/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0167 - accuracy: 0.9945\n",
      "Epoch 00019: val_loss did not improve from 0.02335\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0167 - accuracy: 0.9945 - val_loss: 0.0504 - val_accuracy: 0.9871\n",
      "Epoch 20/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0190 - accuracy: 0.9932\n",
      "Epoch 00020: val_loss did not improve from 0.02335\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0190 - accuracy: 0.9932 - val_loss: 0.0572 - val_accuracy: 0.9768\n",
      "Epoch 21/100\n",
      "35/35 [==============================] - ETA: 0s - loss: 0.0140 - accuracy: 0.9955\n",
      "Epoch 00021: val_loss did not improve from 0.02335\n",
      "35/35 [==============================] - 2s 44ms/step - loss: 0.0140 - accuracy: 0.9955 - val_loss: 0.0329 - val_accuracy: 0.9923\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, batch_size=64, epochs=100, validation_split=0.15, callbacks=[checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9/9 [==============================] - 0s 12ms/step - loss: 0.0943 - accuracy: 0.9826\n",
      "정확도 : 0.98 \n"
     ]
    }
   ],
   "source": [
    "print(\"정확도 : %.2f \" %(model.evaluate(X_test, y_test)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'val_loss', 'acc', 'val_acc'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape [None, 34, 26, 3]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-78fa94e43bd3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./Pictures/imagecrowling/model/open_close_classify2.model'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m \u001b[0mev\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[0;32m    129\u001b[0m           method.__name__))\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1597\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1598\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1599\u001b[1;33m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1600\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1601\u001b[0m               \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    778\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"nonXla\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 780\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    781\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    821\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    694\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 696\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    697\u001b[0m             *args, **kwds))\n\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2853\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2854\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2855\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2856\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2857\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3211\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3213\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3214\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3215\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3063\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3064\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3065\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3066\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3067\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    984\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    598\u001b[0m         \u001b[1;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 600\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    601\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mref\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 973\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    974\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1462 predict_function  *\n        return step_function(self, iterator)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1452 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1211 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2585 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2945 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1445 run_step  **\n        outputs = model.predict_step(data)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1418 predict_step\n        return self(x, training=False)\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:975 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs,\n    C:\\Users\\swc03\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:212 assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer sequential_1 is incompatible with the layer: expected axis -1 of input shape to have value 1 but received input with shape [None, 34, 26, 3]\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob, numpy as np\n",
    "from keras.models import load_model\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "img_dir = './Pictures/imagecrowling/img_test'\n",
    "\n",
    "\n",
    "image_w = 26\n",
    "image_h = 34\n",
    "\n",
    "pixels = image_w * image_h * 1\n",
    "\n",
    "X = []\n",
    "filenames = []\n",
    "files = glob.glob(img_dir+\"/*.jpg\")\n",
    "for i, f in enumerate(files):\n",
    "    img = Image.open(f)\n",
    "    img = img.resize((image_w, image_h))\n",
    "    data = np.asarray(img)\n",
    "\n",
    "    filenames.append(f)\n",
    "    X.append(data)\n",
    "\n",
    "X = np.array(X)\n",
    "X = X.astype(float) / 255\n",
    "model = load_model('./Pictures/imagecrowling/model/open_close_classify2.model')\n",
    "ev=model.predict(X)\n",
    "print(ev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
